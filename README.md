# activation functions

### various activation functions used in machine learning are explained


# various activation functions

* [sigmoid function](sigmoid.rd)
* [softmax funtion](softmax.rd)
* [linear funciton](linear.rd)
* [tanh fucntion](tanh.rd)

# usage
```
python activation.py <function name>
```
```
python activation.py linear

python activation.py sigmoid

python activation.py softmax

python activation.py tanh

python activation.py relu 0 # normal relu

python activation.py relu 1 # noisy relu

python activation.py relu 2 #leaky relu

```
 following graph is drawn based on input range(-100,100) vs respective values based on function

![screenshot_20190208_222004](https://user-images.githubusercontent.com/21342104/52493795-5afb2800-2bf2-11e9-907d-85191e98c4dc.png)
![func 1](https://user-images.githubusercontent.com/21342104/52493690-21c2b800-2bf2-11e9-8db6-e56710ae36e5.png)
![func 2](https://user-images.githubusercontent.com/21342104/52493696-225b4e80-2bf2-11e9-907d-9596c2ed5193.png)
![func 3](https://user-images.githubusercontent.com/21342104/52493697-22f3e500-2bf2-11e9-9f95-a7d825adb9d2.png)
![func 4](https://user-images.githubusercontent.com/21342104/52493699-238c7b80-2bf2-11e9-9ba9-f6890cc99e20.png)
![func 5](https://user-images.githubusercontent.com/21342104/52493712-27b89900-2bf2-11e9-943e-5fc4d8ac98f4.png)
